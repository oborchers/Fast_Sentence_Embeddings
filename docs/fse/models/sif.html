<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>fse.models.sif API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>fse.models.sif</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Author: Oliver Borchers
# Copyright (C) Oliver Borchers

from fse.models.average import Average
from fse.models.utils import compute_principal_components, remove_principal_components

from gensim.models.keyedvectors import KeyedVectors

from numpy import ndarray, float32 as REAL, zeros, isfinite

import logging

logger = logging.getLogger(__name__)


class SIF(Average):
    def __init__(
        self,
        model: KeyedVectors,
        alpha: float = 1e-3,
        components: int = 1,
        cache_size_gb: float = 1.0,
        sv_mapfile_path: str = None,
        wv_mapfile_path: str = None,
        workers: int = 1,
        lang_freq: str = None,
    ):
        &#34;&#34;&#34;Smooth-inverse frequency (SIF) weighted sentence embeddings model. Performs a weighted averaging operation over all
        words in a sentences. After training, the model removes a number of singular vectors.

        The implementation is based on Arora et al. (2017): A Simple but Tough-to-Beat Baseline for Sentence Embeddings.
        For more information, see &lt;https://openreview.net/pdf?id=SyK00v5xx&gt; and &lt;https://github.com/PrincetonML/SIF&gt;

        Parameters
        ----------
        model : :class:`~gensim.models.keyedvectors.KeyedVectors` or :class:`~gensim.models.base_any2vec.BaseWordEmbeddingsModel`
            This object essentially contains the mapping between words and embeddings. To compute the sentence embeddings
            the wv.vocab and wv.vector elements are required.
        alpha : float, optional
            Alpha is the weighting factor used to downweigh each individual word.
        components : int, optional
            Corresponds to the number of singular vectors to remove from the sentence embeddings.
        cache_size_gb : float, optional
            Cache size for computing the singular vectors in GB.
        sv_mapfile_path : str, optional
            Optional path to store the sentence-vectors in for very large datasets. Used for memmap.
        wv_mapfile_path : str, optional
            Optional path to store the word-vectors in for very large datasets. Used for memmap.
            Use sv_mapfile_path and wv_mapfile_path to train disk-to-disk without needing much ram.
        workers : int, optional
            Number of working threads, used for multithreading. For most tasks (few words in a sentence)
            a value of 1 should be more than enough.
        lang_freq : str, optional
            Some pre-trained embeddings, i.e. &#34;GoogleNews-vectors-negative300.bin&#34;, do not contain information about
            the frequency of a word. As the frequency is required for estimating the word weights, we induce
            frequencies into the wv.vocab.count based on :class:`~wordfreq`
            If no frequency information is available, you can choose the language to estimate the frequency.
            See https://github.com/LuminosoInsight/wordfreq

        &#34;&#34;&#34;

        self.alpha = float(alpha)
        self.components = int(components)
        self.cache_size_gb = float(cache_size_gb)
        self.svd_res = None

        if lang_freq is None:
            logger.info(
                &#34;make sure you are using a model with valid word-frequency information. Otherwise use lang_freq argument.&#34;
            )

        super(SIF, self).__init__(
            model=model,
            sv_mapfile_path=sv_mapfile_path,
            wv_mapfile_path=wv_mapfile_path,
            workers=workers,
            lang_freq=lang_freq,
        )

    def _check_parameter_sanity(self):
        &#34;&#34;&#34; Check the sanity of all paramters &#34;&#34;&#34;
        if not all(self.word_weights &lt;= 1.0) or not all(self.word_weights &gt;= 0.0):
            raise ValueError(&#34;For SIF, all word weights must be 0 &lt;= w_weight &lt;= 1&#34;)
        if self.alpha &lt;= 0.0:
            raise ValueError(&#34;Alpha must be greater than zero.&#34;)
        if self.components &lt; 0.0:
            raise ValueError(&#34;Components must be greater or equal zero&#34;)

    def _pre_train_calls(self, **kwargs):
        &#34;&#34;&#34;Function calls to perform before training &#34;&#34;&#34;
        self._compute_sif_weights()

    def _post_train_calls(self):
        &#34;&#34;&#34; Function calls to perform after training, such as computing eigenvectors &#34;&#34;&#34;
        if self.components &gt; 0:
            self.svd_res = compute_principal_components(
                self.sv.vectors,
                components=self.components,
                cache_size_gb=self.cache_size_gb,
            )
            remove_principal_components(
                self.sv.vectors, svd_res=self.svd_res, inplace=True
            )
        else:
            self.svd_res = 0
            logger.info(f&#34;no removal of principal components&#34;)

    def _post_inference_calls(self, output: ndarray, **kwargs):
        &#34;&#34;&#34; Function calls to perform after training &amp; inference &#34;&#34;&#34;
        if self.svd_res is None:
            raise RuntimeError(
                &#34;You must first train the model to obtain SVD components&#34;
            )
        elif self.components &gt; 0:
            remove_principal_components(output, svd_res=self.svd_res, inplace=True)
        else:
            logger.info(f&#34;no removal of principal components&#34;)

    def _check_dtype_santiy(self):
        &#34;&#34;&#34; Check the dtypes of all attributes &#34;&#34;&#34;
        if self.word_weights.dtype != REAL:
            raise TypeError(f&#34;type of word_weights is wrong: {self.word_weights.dtype}&#34;)
        if self.svd_res is not None:
            if self.svd_res[0].dtype != REAL:
                raise TypeError(f&#34;type of svd values is wrong: {self.svd_res[0].dtype}&#34;)
            if self.svd_res[1].dtype != REAL:
                raise TypeError(
                    f&#34;type of svd components is wrong: {self.svd_res[1].dtype}&#34;
                )

    def _compute_sif_weights(self):
        &#34;&#34;&#34; Precomputes the SIF weights for all words in the vocabulary &#34;&#34;&#34;
        logger.info(f&#34;pre-computing SIF weights for {len(self.wv)} words&#34;)
        v = len(self.wv)
        corpus_size = 0

        pw = zeros(v, dtype=REAL)
        for word in self.wv.key_to_index:
            c = self.wv.get_vecattr(word, &#34;count&#34;)
            if c &lt; 0:
                raise ValueError(&#34;vocab count is negative&#34;)
            corpus_size += c
            pw[self.wv.key_to_index[word]] = c
        pw /= corpus_size

        self.word_weights = (self.alpha / (self.alpha + pw)).astype(REAL)

        if not all(isfinite(self.word_weights)) or any(self.word_weights &lt; 0):
            raise RuntimeError(
                &#34;Encountered nan values. &#34;
                &#34;This likely happens because the word frequency information is wrong/missing. &#34;
                &#34;Consider restarting using lang_freq argument to infer frequency. &#34;
            )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="fse.models.sif.SIF"><code class="flex name class">
<span>class <span class="ident">SIF</span></span>
<span>(</span><span>model: gensim.models.keyedvectors.KeyedVectors, alpha: float = 0.001, components: int = 1, cache_size_gb: float = 1.0, sv_mapfile_path: str = None, wv_mapfile_path: str = None, workers: int = 1, lang_freq: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Train, use and evaluate averaged sentence vectors.</p>
<p>The model can be stored/loaded via its :meth:<code>~fse.models.average.Average.save</code> and
:meth:<code>~fse.models.average.Average.load</code> methods.</p>
<p>Some important attributes are the following:</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>wv</code></strong> :&ensp;<code>:class:</code>~gensim.models.keyedvectors.KeyedVectors``</dt>
<dd>This object essentially contains the mapping between words and embeddings. After training, it can be used
directly to query those embeddings in various ways. See the module level docstring for examples.</dd>
<dt><strong><code>sv</code></strong> :&ensp;<code>:class:</code>~fse.models.sentencevectors.SentenceVectors``</dt>
<dd>This object contains the sentence vectors inferred from the training data. There will be one such vector
for each unique docusentence supplied during training. They may be individually accessed using the index.</dd>
<dt><strong><code>prep</code></strong> :&ensp;<code>:class:</code>~fse.models.base_s2v.BaseSentence2VecPreparer``</dt>
<dd>The prep object is used to transform and initialize the sv.vectors. Aditionally, it can be used
to move the vectors to disk for training with memmap.</dd>
</dl>
<p>Smooth-inverse frequency (SIF) weighted sentence embeddings model. Performs a weighted averaging operation over all
words in a sentences. After training, the model removes a number of singular vectors.</p>
<p>The implementation is based on Arora et al. (2017): A Simple but Tough-to-Beat Baseline for Sentence Embeddings.
For more information, see <a href="https://openreview.net/pdf?id=SyK00v5xx">https://openreview.net/pdf?id=SyK00v5xx</a> and <a href="https://github.com/PrincetonML/SIF">https://github.com/PrincetonML/SIF</a></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>:class:</code>~gensim.models.keyedvectors.KeyedVectors<code>or `:class:`~gensim.models.base_any2vec.BaseWordEmbeddingsModel</code></dt>
<dd>This object essentially contains the mapping between words and embeddings. To compute the sentence embeddings
the wv.vocab and wv.vector elements are required.</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Alpha is the weighting factor used to downweigh each individual word.</dd>
<dt><strong><code>components</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Corresponds to the number of singular vectors to remove from the sentence embeddings.</dd>
<dt><strong><code>cache_size_gb</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Cache size for computing the singular vectors in GB.</dd>
<dt><strong><code>sv_mapfile_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Optional path to store the sentence-vectors in for very large datasets. Used for memmap.</dd>
<dt><strong><code>wv_mapfile_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Optional path to store the word-vectors in for very large datasets. Used for memmap.
Use sv_mapfile_path and wv_mapfile_path to train disk-to-disk without needing much ram.</dd>
<dt><strong><code>workers</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of working threads, used for multithreading. For most tasks (few words in a sentence)
a value of 1 should be more than enough.</dd>
<dt><strong><code>lang_freq</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Some pre-trained embeddings, i.e. "GoogleNews-vectors-negative300.bin", do not contain information about
the frequency of a word. As the frequency is required for estimating the word weights, we induce
frequencies into the wv.vocab.count based on :class:<code>~wordfreq</code>
If no frequency information is available, you can choose the language to estimate the frequency.
See <a href="https://github.com/LuminosoInsight/wordfreq">https://github.com/LuminosoInsight/wordfreq</a></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SIF(Average):
    def __init__(
        self,
        model: KeyedVectors,
        alpha: float = 1e-3,
        components: int = 1,
        cache_size_gb: float = 1.0,
        sv_mapfile_path: str = None,
        wv_mapfile_path: str = None,
        workers: int = 1,
        lang_freq: str = None,
    ):
        &#34;&#34;&#34;Smooth-inverse frequency (SIF) weighted sentence embeddings model. Performs a weighted averaging operation over all
        words in a sentences. After training, the model removes a number of singular vectors.

        The implementation is based on Arora et al. (2017): A Simple but Tough-to-Beat Baseline for Sentence Embeddings.
        For more information, see &lt;https://openreview.net/pdf?id=SyK00v5xx&gt; and &lt;https://github.com/PrincetonML/SIF&gt;

        Parameters
        ----------
        model : :class:`~gensim.models.keyedvectors.KeyedVectors` or :class:`~gensim.models.base_any2vec.BaseWordEmbeddingsModel`
            This object essentially contains the mapping between words and embeddings. To compute the sentence embeddings
            the wv.vocab and wv.vector elements are required.
        alpha : float, optional
            Alpha is the weighting factor used to downweigh each individual word.
        components : int, optional
            Corresponds to the number of singular vectors to remove from the sentence embeddings.
        cache_size_gb : float, optional
            Cache size for computing the singular vectors in GB.
        sv_mapfile_path : str, optional
            Optional path to store the sentence-vectors in for very large datasets. Used for memmap.
        wv_mapfile_path : str, optional
            Optional path to store the word-vectors in for very large datasets. Used for memmap.
            Use sv_mapfile_path and wv_mapfile_path to train disk-to-disk without needing much ram.
        workers : int, optional
            Number of working threads, used for multithreading. For most tasks (few words in a sentence)
            a value of 1 should be more than enough.
        lang_freq : str, optional
            Some pre-trained embeddings, i.e. &#34;GoogleNews-vectors-negative300.bin&#34;, do not contain information about
            the frequency of a word. As the frequency is required for estimating the word weights, we induce
            frequencies into the wv.vocab.count based on :class:`~wordfreq`
            If no frequency information is available, you can choose the language to estimate the frequency.
            See https://github.com/LuminosoInsight/wordfreq

        &#34;&#34;&#34;

        self.alpha = float(alpha)
        self.components = int(components)
        self.cache_size_gb = float(cache_size_gb)
        self.svd_res = None

        if lang_freq is None:
            logger.info(
                &#34;make sure you are using a model with valid word-frequency information. Otherwise use lang_freq argument.&#34;
            )

        super(SIF, self).__init__(
            model=model,
            sv_mapfile_path=sv_mapfile_path,
            wv_mapfile_path=wv_mapfile_path,
            workers=workers,
            lang_freq=lang_freq,
        )

    def _check_parameter_sanity(self):
        &#34;&#34;&#34; Check the sanity of all paramters &#34;&#34;&#34;
        if not all(self.word_weights &lt;= 1.0) or not all(self.word_weights &gt;= 0.0):
            raise ValueError(&#34;For SIF, all word weights must be 0 &lt;= w_weight &lt;= 1&#34;)
        if self.alpha &lt;= 0.0:
            raise ValueError(&#34;Alpha must be greater than zero.&#34;)
        if self.components &lt; 0.0:
            raise ValueError(&#34;Components must be greater or equal zero&#34;)

    def _pre_train_calls(self, **kwargs):
        &#34;&#34;&#34;Function calls to perform before training &#34;&#34;&#34;
        self._compute_sif_weights()

    def _post_train_calls(self):
        &#34;&#34;&#34; Function calls to perform after training, such as computing eigenvectors &#34;&#34;&#34;
        if self.components &gt; 0:
            self.svd_res = compute_principal_components(
                self.sv.vectors,
                components=self.components,
                cache_size_gb=self.cache_size_gb,
            )
            remove_principal_components(
                self.sv.vectors, svd_res=self.svd_res, inplace=True
            )
        else:
            self.svd_res = 0
            logger.info(f&#34;no removal of principal components&#34;)

    def _post_inference_calls(self, output: ndarray, **kwargs):
        &#34;&#34;&#34; Function calls to perform after training &amp; inference &#34;&#34;&#34;
        if self.svd_res is None:
            raise RuntimeError(
                &#34;You must first train the model to obtain SVD components&#34;
            )
        elif self.components &gt; 0:
            remove_principal_components(output, svd_res=self.svd_res, inplace=True)
        else:
            logger.info(f&#34;no removal of principal components&#34;)

    def _check_dtype_santiy(self):
        &#34;&#34;&#34; Check the dtypes of all attributes &#34;&#34;&#34;
        if self.word_weights.dtype != REAL:
            raise TypeError(f&#34;type of word_weights is wrong: {self.word_weights.dtype}&#34;)
        if self.svd_res is not None:
            if self.svd_res[0].dtype != REAL:
                raise TypeError(f&#34;type of svd values is wrong: {self.svd_res[0].dtype}&#34;)
            if self.svd_res[1].dtype != REAL:
                raise TypeError(
                    f&#34;type of svd components is wrong: {self.svd_res[1].dtype}&#34;
                )

    def _compute_sif_weights(self):
        &#34;&#34;&#34; Precomputes the SIF weights for all words in the vocabulary &#34;&#34;&#34;
        logger.info(f&#34;pre-computing SIF weights for {len(self.wv)} words&#34;)
        v = len(self.wv)
        corpus_size = 0

        pw = zeros(v, dtype=REAL)
        for word in self.wv.key_to_index:
            c = self.wv.get_vecattr(word, &#34;count&#34;)
            if c &lt; 0:
                raise ValueError(&#34;vocab count is negative&#34;)
            corpus_size += c
            pw[self.wv.key_to_index[word]] = c
        pw /= corpus_size

        self.word_weights = (self.alpha / (self.alpha + pw)).astype(REAL)

        if not all(isfinite(self.word_weights)) or any(self.word_weights &lt; 0):
            raise RuntimeError(
                &#34;Encountered nan values. &#34;
                &#34;This likely happens because the word frequency information is wrong/missing. &#34;
                &#34;Consider restarting using lang_freq argument to infer frequency. &#34;
            )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="fse.models.average.Average" href="average.html#fse.models.average.Average">Average</a></li>
<li><a title="fse.models.base_s2v.BaseSentence2VecModel" href="base_s2v.html#fse.models.base_s2v.BaseSentence2VecModel">BaseSentence2VecModel</a></li>
<li>gensim.utils.SaveLoad</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="fse.models.average.Average" href="average.html#fse.models.average.Average">Average</a></b></code>:
<ul class="hlist">
<li><code><a title="fse.models.average.Average.estimate_memory" href="base_s2v.html#fse.models.base_s2v.BaseSentence2VecModel.estimate_memory">estimate_memory</a></code></li>
<li><code><a title="fse.models.average.Average.infer" href="base_s2v.html#fse.models.base_s2v.BaseSentence2VecModel.infer">infer</a></code></li>
<li><code><a title="fse.models.average.Average.load" href="base_s2v.html#fse.models.base_s2v.BaseSentence2VecModel.load">load</a></code></li>
<li><code><a title="fse.models.average.Average.save" href="base_s2v.html#fse.models.base_s2v.BaseSentence2VecModel.save">save</a></code></li>
<li><code><a title="fse.models.average.Average.scan_sentences" href="base_s2v.html#fse.models.base_s2v.BaseSentence2VecModel.scan_sentences">scan_sentences</a></code></li>
<li><code><a title="fse.models.average.Average.train" href="base_s2v.html#fse.models.base_s2v.BaseSentence2VecModel.train">train</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="fse.models" href="index.html">fse.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="fse.models.sif.SIF" href="#fse.models.sif.SIF">SIF</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>