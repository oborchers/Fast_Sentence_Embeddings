<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>fse.models.average API documentation</title>
<meta name="description" content="This module implements the base class to compute average representations for sentences, using highly optimized C routines,
data streaming and Pythonic â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>fse.models.average</code></h1>
</header>
<section id="section-intro">
<p>This module implements the base class to compute average representations for sentences, using highly optimized C routines,
data streaming and Pythonic interfaces.</p>
<p>The implementation is based on Iyyer et al. (2015): Deep Unordered Composition Rivals Syntactic Methods for Text Classification.
For more information, see <a href="https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf">https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf</a>.</p>
<p>The training algorithms is based on the Gensim implementation of Word2Vec, FastText, and Doc2Vec.
For more information, see: :class:<code>~gensim.models.word2vec.Word2Vec</code>, :class:<code>~gensim.models.fasttext.FastText</code>, or
:class:<code>~gensim.models.doc2vec.Doc2Vec</code>.</p>
<p>Initialize and train a :class:<code>~fse.models.sentence2vec.Sentence2Vec</code> model</p>
<div class="admonition sourcecode">
<p class="admonition-title">Sourcecode:&ensp;pycon</p>
<blockquote>
<blockquote>
<blockquote>
<p>from gensim.models.word2vec import Word2Vec
sentences = [["cat", "say", "meow"], ["dog", "say", "woof"]]
model = Word2Vec(sentences, min_count=1, vector_size=20)</p>
<p>from fse.models.average import Average
<br>
avg = Average(model)
avg.train([(s, i) for i, s in enumerate(sentences)])
avg.sv.vectors.shape
(2, 20)</p>
</blockquote>
</blockquote>
</blockquote>
</div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Author: Oliver Borchers
# Copyright (C) Oliver Borchers

&#34;&#34;&#34;This module implements the base class to compute average representations for sentences, using highly optimized C routines,
data streaming and Pythonic interfaces.

The implementation is based on Iyyer et al. (2015): Deep Unordered Composition Rivals Syntactic Methods for Text Classification.
For more information, see &lt;https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf&gt;.

The training algorithms is based on the Gensim implementation of Word2Vec, FastText, and Doc2Vec. 
For more information, see: :class:`~gensim.models.word2vec.Word2Vec`, :class:`~gensim.models.fasttext.FastText`, or
:class:`~gensim.models.doc2vec.Doc2Vec`.

Initialize and train a :class:`~fse.models.sentence2vec.Sentence2Vec` model

.. sourcecode:: pycon

        &gt;&gt;&gt; from gensim.models.word2vec import Word2Vec
        &gt;&gt;&gt; sentences = [[&#34;cat&#34;, &#34;say&#34;, &#34;meow&#34;], [&#34;dog&#34;, &#34;say&#34;, &#34;woof&#34;]]
        &gt;&gt;&gt; model = Word2Vec(sentences, min_count=1, vector_size=20)

        &gt;&gt;&gt; from fse.models.average import Average        
        &gt;&gt;&gt; avg = Average(model)
        &gt;&gt;&gt; avg.train([(s, i) for i, s in enumerate(sentences)])
        &gt;&gt;&gt; avg.sv.vectors.shape
        (2, 20)

&#34;&#34;&#34;

from __future__ import division

from fse.models.base_s2v import BaseSentence2VecModel

from gensim.models.keyedvectors import KeyedVectors
from gensim.models.fasttext import ft_ngram_hashes

from numpy import (
    ndarray,
    float32 as REAL,
    sum as np_sum,
    multiply as np_mult,
    zeros,
    max as np_max,
)

from typing import List, Tuple

import logging

logger = logging.getLogger(__name__)


def train_average_np(
    model: BaseSentence2VecModel,
    indexed_sentences: List[tuple],
    target: ndarray,
    memory: ndarray,
) -&gt; Tuple[int, int]:
    &#34;&#34;&#34;Training on a sequence of sentences and update the target ndarray.

    Called internally from :meth:`~fse.models.average.Average._do_train_job`.

    Warnings
    --------
    This is the non-optimized, pure Python version. If you have a C compiler,
    fse will use an optimized code path from :mod:`fse.models.average_inner` instead.

    Parameters
    ----------
    model : :class:`~fse.models.base_s2v.BaseSentence2VecModel`
        The BaseSentence2VecModel model instance.
    indexed_sentences : iterable of tuple
        The sentences used to train the model.
    target : ndarray
        The target ndarray. We use the index from indexed_sentences
        to write into the corresponding row of target.
    memory : ndarray
        Private memory for each working thread

    Returns
    -------
    int, int
        Number of effective sentences (non-zero) and effective words in the vocabulary used
        during training the sentence embedding.

    &#34;&#34;&#34;
    size = model.wv.vector_size

    w_vectors = model.wv.vectors
    w_weights = model.word_weights

    s_vectors = target

    is_ft = model.is_ft

    mem = memory[0]

    if is_ft:
        # NOTE: For Fasttext: Use wv.vectors_vocab
        # Using the wv.vectors from fasttext had horrible effects on the sts results
        # I suspect this is because the wv.vectors are based on the averages of
        # wv.vectors_vocab + wv.vectors_ngrams, which will all point into very
        # similar directions.
        max_ngrams = model.batch_ngrams
        w_vectors = model.wv.vectors_vocab
        ngram_vectors = model.wv.vectors_ngrams
        min_n = model.wv.min_n
        max_n = model.wv.max_n
        bucket = model.wv.bucket
        oov_weight = np_max(w_weights)

    eff_sentences, eff_words = 0, 0

    if not is_ft:
        for obj in indexed_sentences:
            mem.fill(0.0)
            sent = obj[0]
            sent_adr = obj[1]

            word_indices = [
                model.wv.key_to_index[word]
                for word in sent
                if word in model.wv.key_to_index
            ]
            eff_sentences += 1
            if not len(word_indices):
                continue
            eff_words += len(word_indices)

            mem += np_sum(
                np_mult(w_vectors[word_indices], w_weights[word_indices][:, None]),
                axis=0,
            )
            mem *= 1 / len(word_indices)
            s_vectors[sent_adr] = mem.astype(REAL)
    else:
        for obj in indexed_sentences:
            mem.fill(0.0)
            sent = obj[0]
            sent_adr = obj[1]

            if not len(sent):
                continue
            mem = zeros(size, dtype=REAL)

            eff_sentences += 1
            eff_words += len(sent)  # Counts everything in the sentence

            for word in sent:
                if word in model.wv.key_to_index:
                    word_index = model.wv.key_to_index[word]
                    mem += w_vectors[word_index] * w_weights[word_index]
                else:
                    ngram_hashes = ft_ngram_hashes(word, min_n, max_n, bucket)[
                        :max_ngrams
                    ]
                    if len(ngram_hashes) == 0:
                        continue
                    mem += oov_weight * (
                        np_sum(ngram_vectors[ngram_hashes], axis=0) / len(ngram_hashes)
                    )
                # Implicit addition of zero if oov does not contain any ngrams
            s_vectors[sent_adr] = mem / len(sent)

    return eff_sentences, eff_words


try:
    from fse.models.average_inner import train_average_cy
    from fse.models.average_inner import (
        FAST_VERSION,
        MAX_WORDS_IN_BATCH,
        MAX_NGRAMS_IN_BATCH,
    )

    train_average = train_average_cy
except ImportError:
    FAST_VERSION = -1
    MAX_WORDS_IN_BATCH = 10000
    MAX_NGRAMS_IN_BATCH = 40
    train_average = train_average_np


class Average(BaseSentence2VecModel):
    &#34;&#34;&#34;Train, use and evaluate averaged sentence vectors.

    The model can be stored/loaded via its :meth:`~fse.models.average.Average.save` and
    :meth:`~fse.models.average.Average.load` methods.

    Some important attributes are the following:

    Attributes
    ----------
    wv : :class:`~gensim.models.keyedvectors.KeyedVectors`
        This object essentially contains the mapping between words and embeddings. After training, it can be used
        directly to query those embeddings in various ways. See the module level docstring for examples.

    sv : :class:`~fse.models.sentencevectors.SentenceVectors`
        This object contains the sentence vectors inferred from the training data. There will be one such vector
        for each unique docusentence supplied during training. They may be individually accessed using the index.

    prep : :class:`~fse.models.base_s2v.BaseSentence2VecPreparer`
        The prep object is used to transform and initialize the sv.vectors. Aditionally, it can be used
        to move the vectors to disk for training with memmap.

    &#34;&#34;&#34;

    def __init__(
        self,
        model: KeyedVectors,
        sv_mapfile_path: str = None,
        wv_mapfile_path: str = None,
        workers: int = 1,
        lang_freq: str = None,
        **kwargs
    ):
        &#34;&#34;&#34;Average (unweighted) sentence embeddings model. Performs a simple averaging operation over all
        words in a sentences without further transformation.

        The implementation is based on Iyyer et al. (2015): Deep Unordered Composition Rivals Syntactic Methods for Text Classification.
        For more information, see &lt;https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf&gt;.

        Parameters
        ----------
        model : :class:`~gensim.models.keyedvectors.KeyedVectors` or :class:`~gensim.models.base_any2vec.BaseWordEmbeddingsModel`
            This object essentially contains the mapping between words and embeddings. To compute the sentence embeddings
            the wv.vocab and wv.vector elements are required.
        sv_mapfile_path : str, optional
            Optional path to store the sentence-vectors in for very large datasets. Used for memmap.
        wv_mapfile_path : str, optional
            Optional path to store the word-vectors in for very large datasets. Used for memmap.
            Use sv_mapfile_path and wv_mapfile_path to train disk-to-disk without needing much ram.
        workers : int, optional
            Number of working threads, used for multithreading. For most tasks (few words in a sentence)
            a value of 1 should be more than enough.
        lang_freq : str, optional
            Some pre-trained embeddings, i.e. &#34;GoogleNews-vectors-negative300.bin&#34;, do not contain information about
            the frequency of a word. As the frequency is required for estimating the word weights, we induce
            frequencies into the wv.vocab.count based on :class:`~wordfreq`
            If no frequency information is available, you can choose the language to estimate the frequency.
            See https://github.com/LuminosoInsight/wordfreq

        &#34;&#34;&#34;

        super(Average, self).__init__(
            model=model,
            sv_mapfile_path=sv_mapfile_path,
            wv_mapfile_path=wv_mapfile_path,
            workers=workers,
            lang_freq=lang_freq,
            batch_words=MAX_WORDS_IN_BATCH,
            batch_ngrams=MAX_NGRAMS_IN_BATCH,
            fast_version=FAST_VERSION,
        )

    def _do_train_job(
        self, data_iterable: List[tuple], target: ndarray, memory: ndarray
    ) -&gt; Tuple[int, int]:
        &#34;&#34;&#34; Internal routine which is called on training and performs averaging for all entries in the iterable &#34;&#34;&#34;
        eff_sentences, eff_words = train_average(
            model=self, indexed_sentences=data_iterable, target=target, memory=memory
        )
        return eff_sentences, eff_words

    def _check_parameter_sanity(self, **kwargs):
        &#34;&#34;&#34; Check the sanity of all child paramters &#34;&#34;&#34;
        if not all(self.word_weights == 1.0):
            raise ValueError(&#34;All word weights must equal one for averaging&#34;)

    def _pre_train_calls(self, **kwargs):
        &#34;&#34;&#34;Function calls to perform before training &#34;&#34;&#34;
        pass

    def _post_train_calls(self, **kwargs):
        &#34;&#34;&#34; Function calls to perform after training, such as computing eigenvectors &#34;&#34;&#34;
        pass

    def _post_inference_calls(self, **kwargs):
        &#34;&#34;&#34;Function calls to perform after training &amp; inference
        Examples include the removal of components
        &#34;&#34;&#34;
        pass

    def _check_dtype_santiy(self, **kwargs):
        &#34;&#34;&#34; Check the dtypes of all child attributes&#34;&#34;&#34;
        pass</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="fse.models.average.train_average_np"><code class="name flex">
<span>def <span class="ident">train_average_np</span></span>(<span>model:Â <a title="fse.models.base_s2v.BaseSentence2VecModel" href="base_s2v.html#fse.models.base_s2v.BaseSentence2VecModel">BaseSentence2VecModel</a>, indexed_sentences:Â List[tuple], target:Â numpy.ndarray, memory:Â numpy.ndarray) â€‘>Â Tuple[int,Â int]</span>
</code></dt>
<dd>
<div class="desc"><p>Training on a sequence of sentences and update the target ndarray.</p>
<p>Called internally from :meth:<code>~fse.models.average.Average._do_train_job</code>.</p>
<h2 id="warnings">Warnings</h2>
<p>This is the non-optimized, pure Python version. If you have a C compiler,
fse will use an optimized code path from :mod:<code><a title="fse.models.average_inner" href="average_inner.html">fse.models.average_inner</a></code> instead.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>:class:</code>~fse.models.base_s2v.BaseSentence2VecModel``</dt>
<dd>The BaseSentence2VecModel model instance.</dd>
<dt><strong><code>indexed_sentences</code></strong> :&ensp;<code>iterable</code> of <code>tuple</code></dt>
<dd>The sentences used to train the model.</dd>
<dt><strong><code>target</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>The target ndarray. We use the index from indexed_sentences
to write into the corresponding row of target.</dd>
<dt><strong><code>memory</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Private memory for each working thread</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int, int</code></dt>
<dd>Number of effective sentences (non-zero) and effective words in the vocabulary used
during training the sentence embedding.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_average_np(
    model: BaseSentence2VecModel,
    indexed_sentences: List[tuple],
    target: ndarray,
    memory: ndarray,
) -&gt; Tuple[int, int]:
    &#34;&#34;&#34;Training on a sequence of sentences and update the target ndarray.

    Called internally from :meth:`~fse.models.average.Average._do_train_job`.

    Warnings
    --------
    This is the non-optimized, pure Python version. If you have a C compiler,
    fse will use an optimized code path from :mod:`fse.models.average_inner` instead.

    Parameters
    ----------
    model : :class:`~fse.models.base_s2v.BaseSentence2VecModel`
        The BaseSentence2VecModel model instance.
    indexed_sentences : iterable of tuple
        The sentences used to train the model.
    target : ndarray
        The target ndarray. We use the index from indexed_sentences
        to write into the corresponding row of target.
    memory : ndarray
        Private memory for each working thread

    Returns
    -------
    int, int
        Number of effective sentences (non-zero) and effective words in the vocabulary used
        during training the sentence embedding.

    &#34;&#34;&#34;
    size = model.wv.vector_size

    w_vectors = model.wv.vectors
    w_weights = model.word_weights

    s_vectors = target

    is_ft = model.is_ft

    mem = memory[0]

    if is_ft:
        # NOTE: For Fasttext: Use wv.vectors_vocab
        # Using the wv.vectors from fasttext had horrible effects on the sts results
        # I suspect this is because the wv.vectors are based on the averages of
        # wv.vectors_vocab + wv.vectors_ngrams, which will all point into very
        # similar directions.
        max_ngrams = model.batch_ngrams
        w_vectors = model.wv.vectors_vocab
        ngram_vectors = model.wv.vectors_ngrams
        min_n = model.wv.min_n
        max_n = model.wv.max_n
        bucket = model.wv.bucket
        oov_weight = np_max(w_weights)

    eff_sentences, eff_words = 0, 0

    if not is_ft:
        for obj in indexed_sentences:
            mem.fill(0.0)
            sent = obj[0]
            sent_adr = obj[1]

            word_indices = [
                model.wv.key_to_index[word]
                for word in sent
                if word in model.wv.key_to_index
            ]
            eff_sentences += 1
            if not len(word_indices):
                continue
            eff_words += len(word_indices)

            mem += np_sum(
                np_mult(w_vectors[word_indices], w_weights[word_indices][:, None]),
                axis=0,
            )
            mem *= 1 / len(word_indices)
            s_vectors[sent_adr] = mem.astype(REAL)
    else:
        for obj in indexed_sentences:
            mem.fill(0.0)
            sent = obj[0]
            sent_adr = obj[1]

            if not len(sent):
                continue
            mem = zeros(size, dtype=REAL)

            eff_sentences += 1
            eff_words += len(sent)  # Counts everything in the sentence

            for word in sent:
                if word in model.wv.key_to_index:
                    word_index = model.wv.key_to_index[word]
                    mem += w_vectors[word_index] * w_weights[word_index]
                else:
                    ngram_hashes = ft_ngram_hashes(word, min_n, max_n, bucket)[
                        :max_ngrams
                    ]
                    if len(ngram_hashes) == 0:
                        continue
                    mem += oov_weight * (
                        np_sum(ngram_vectors[ngram_hashes], axis=0) / len(ngram_hashes)
                    )
                # Implicit addition of zero if oov does not contain any ngrams
            s_vectors[sent_adr] = mem / len(sent)

    return eff_sentences, eff_words</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="fse.models.average.Average"><code class="flex name class">
<span>class <span class="ident">Average</span></span>
<span>(</span><span>model:Â gensim.models.keyedvectors.KeyedVectors, sv_mapfile_path:Â strÂ =Â None, wv_mapfile_path:Â strÂ =Â None, workers:Â intÂ =Â 1, lang_freq:Â strÂ =Â None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Train, use and evaluate averaged sentence vectors.</p>
<p>The model can be stored/loaded via its :meth:<code>~fse.models.average.Average.save</code> and
:meth:<code>~fse.models.average.Average.load</code> methods.</p>
<p>Some important attributes are the following:</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>wv</code></strong> :&ensp;<code>:class:</code>~gensim.models.keyedvectors.KeyedVectors``</dt>
<dd>This object essentially contains the mapping between words and embeddings. After training, it can be used
directly to query those embeddings in various ways. See the module level docstring for examples.</dd>
<dt><strong><code>sv</code></strong> :&ensp;<code>:class:</code>~fse.models.sentencevectors.SentenceVectors``</dt>
<dd>This object contains the sentence vectors inferred from the training data. There will be one such vector
for each unique docusentence supplied during training. They may be individually accessed using the index.</dd>
<dt><strong><code>prep</code></strong> :&ensp;<code>:class:</code>~fse.models.base_s2v.BaseSentence2VecPreparer``</dt>
<dd>The prep object is used to transform and initialize the sv.vectors. Aditionally, it can be used
to move the vectors to disk for training with memmap.</dd>
</dl>
<p>Average (unweighted) sentence embeddings model. Performs a simple averaging operation over all
words in a sentences without further transformation.</p>
<p>The implementation is based on Iyyer et al. (2015): Deep Unordered Composition Rivals Syntactic Methods for Text Classification.
For more information, see <a href="https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf">https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf</a>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>:class:</code>~gensim.models.keyedvectors.KeyedVectors<code>or `:class:`~gensim.models.base_any2vec.BaseWordEmbeddingsModel</code></dt>
<dd>This object essentially contains the mapping between words and embeddings. To compute the sentence embeddings
the wv.vocab and wv.vector elements are required.</dd>
<dt><strong><code>sv_mapfile_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Optional path to store the sentence-vectors in for very large datasets. Used for memmap.</dd>
<dt><strong><code>wv_mapfile_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Optional path to store the word-vectors in for very large datasets. Used for memmap.
Use sv_mapfile_path and wv_mapfile_path to train disk-to-disk without needing much ram.</dd>
<dt><strong><code>workers</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of working threads, used for multithreading. For most tasks (few words in a sentence)
a value of 1 should be more than enough.</dd>
<dt><strong><code>lang_freq</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Some pre-trained embeddings, i.e. "GoogleNews-vectors-negative300.bin", do not contain information about
the frequency of a word. As the frequency is required for estimating the word weights, we induce
frequencies into the wv.vocab.count based on :class:<code>~wordfreq</code>
If no frequency information is available, you can choose the language to estimate the frequency.
See <a href="https://github.com/LuminosoInsight/wordfreq">https://github.com/LuminosoInsight/wordfreq</a></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Average(BaseSentence2VecModel):
    &#34;&#34;&#34;Train, use and evaluate averaged sentence vectors.

    The model can be stored/loaded via its :meth:`~fse.models.average.Average.save` and
    :meth:`~fse.models.average.Average.load` methods.

    Some important attributes are the following:

    Attributes
    ----------
    wv : :class:`~gensim.models.keyedvectors.KeyedVectors`
        This object essentially contains the mapping between words and embeddings. After training, it can be used
        directly to query those embeddings in various ways. See the module level docstring for examples.

    sv : :class:`~fse.models.sentencevectors.SentenceVectors`
        This object contains the sentence vectors inferred from the training data. There will be one such vector
        for each unique docusentence supplied during training. They may be individually accessed using the index.

    prep : :class:`~fse.models.base_s2v.BaseSentence2VecPreparer`
        The prep object is used to transform and initialize the sv.vectors. Aditionally, it can be used
        to move the vectors to disk for training with memmap.

    &#34;&#34;&#34;

    def __init__(
        self,
        model: KeyedVectors,
        sv_mapfile_path: str = None,
        wv_mapfile_path: str = None,
        workers: int = 1,
        lang_freq: str = None,
        **kwargs
    ):
        &#34;&#34;&#34;Average (unweighted) sentence embeddings model. Performs a simple averaging operation over all
        words in a sentences without further transformation.

        The implementation is based on Iyyer et al. (2015): Deep Unordered Composition Rivals Syntactic Methods for Text Classification.
        For more information, see &lt;https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf&gt;.

        Parameters
        ----------
        model : :class:`~gensim.models.keyedvectors.KeyedVectors` or :class:`~gensim.models.base_any2vec.BaseWordEmbeddingsModel`
            This object essentially contains the mapping between words and embeddings. To compute the sentence embeddings
            the wv.vocab and wv.vector elements are required.
        sv_mapfile_path : str, optional
            Optional path to store the sentence-vectors in for very large datasets. Used for memmap.
        wv_mapfile_path : str, optional
            Optional path to store the word-vectors in for very large datasets. Used for memmap.
            Use sv_mapfile_path and wv_mapfile_path to train disk-to-disk without needing much ram.
        workers : int, optional
            Number of working threads, used for multithreading. For most tasks (few words in a sentence)
            a value of 1 should be more than enough.
        lang_freq : str, optional
            Some pre-trained embeddings, i.e. &#34;GoogleNews-vectors-negative300.bin&#34;, do not contain information about
            the frequency of a word. As the frequency is required for estimating the word weights, we induce
            frequencies into the wv.vocab.count based on :class:`~wordfreq`
            If no frequency information is available, you can choose the language to estimate the frequency.
            See https://github.com/LuminosoInsight/wordfreq

        &#34;&#34;&#34;

        super(Average, self).__init__(
            model=model,
            sv_mapfile_path=sv_mapfile_path,
            wv_mapfile_path=wv_mapfile_path,
            workers=workers,
            lang_freq=lang_freq,
            batch_words=MAX_WORDS_IN_BATCH,
            batch_ngrams=MAX_NGRAMS_IN_BATCH,
            fast_version=FAST_VERSION,
        )

    def _do_train_job(
        self, data_iterable: List[tuple], target: ndarray, memory: ndarray
    ) -&gt; Tuple[int, int]:
        &#34;&#34;&#34; Internal routine which is called on training and performs averaging for all entries in the iterable &#34;&#34;&#34;
        eff_sentences, eff_words = train_average(
            model=self, indexed_sentences=data_iterable, target=target, memory=memory
        )
        return eff_sentences, eff_words

    def _check_parameter_sanity(self, **kwargs):
        &#34;&#34;&#34; Check the sanity of all child paramters &#34;&#34;&#34;
        if not all(self.word_weights == 1.0):
            raise ValueError(&#34;All word weights must equal one for averaging&#34;)

    def _pre_train_calls(self, **kwargs):
        &#34;&#34;&#34;Function calls to perform before training &#34;&#34;&#34;
        pass

    def _post_train_calls(self, **kwargs):
        &#34;&#34;&#34; Function calls to perform after training, such as computing eigenvectors &#34;&#34;&#34;
        pass

    def _post_inference_calls(self, **kwargs):
        &#34;&#34;&#34;Function calls to perform after training &amp; inference
        Examples include the removal of components
        &#34;&#34;&#34;
        pass

    def _check_dtype_santiy(self, **kwargs):
        &#34;&#34;&#34; Check the dtypes of all child attributes&#34;&#34;&#34;
        pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="fse.models.base_s2v.BaseSentence2VecModel" href="base_s2v.html#fse.models.base_s2v.BaseSentence2VecModel">BaseSentence2VecModel</a></li>
<li>gensim.utils.SaveLoad</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="fse.models.sif.SIF" href="sif.html#fse.models.sif.SIF">SIF</a></li>
<li><a title="fse.models.usif.uSIF" href="usif.html#fse.models.usif.uSIF">uSIF</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="fse.models.base_s2v.BaseSentence2VecModel" href="base_s2v.html#fse.models.base_s2v.BaseSentence2VecModel">BaseSentence2VecModel</a></b></code>:
<ul class="hlist">
<li><code><a title="fse.models.base_s2v.BaseSentence2VecModel.estimate_memory" href="base_s2v.html#fse.models.base_s2v.BaseSentence2VecModel.estimate_memory">estimate_memory</a></code></li>
<li><code><a title="fse.models.base_s2v.BaseSentence2VecModel.infer" href="base_s2v.html#fse.models.base_s2v.BaseSentence2VecModel.infer">infer</a></code></li>
<li><code><a title="fse.models.base_s2v.BaseSentence2VecModel.load" href="base_s2v.html#fse.models.base_s2v.BaseSentence2VecModel.load">load</a></code></li>
<li><code><a title="fse.models.base_s2v.BaseSentence2VecModel.save" href="base_s2v.html#fse.models.base_s2v.BaseSentence2VecModel.save">save</a></code></li>
<li><code><a title="fse.models.base_s2v.BaseSentence2VecModel.scan_sentences" href="base_s2v.html#fse.models.base_s2v.BaseSentence2VecModel.scan_sentences">scan_sentences</a></code></li>
<li><code><a title="fse.models.base_s2v.BaseSentence2VecModel.train" href="base_s2v.html#fse.models.base_s2v.BaseSentence2VecModel.train">train</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="fse.models" href="index.html">fse.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="fse.models.average.train_average_np" href="#fse.models.average.train_average_np">train_average_np</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="fse.models.average.Average" href="#fse.models.average.Average">Average</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>